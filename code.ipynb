{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69854aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEVELOPMET ROAD\n",
    "\n",
    "#1 THEORY AND PREPARATION\n",
    "#  go over the theory and explain what we are going to build prepare our data and environment\n",
    "\n",
    "#TRAIN MODEL\n",
    "#  train our model on the data we have downloaded\n",
    "\n",
    "#CREATE THE GUI\n",
    "#   Create a GUI to show our results in live live time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0571ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEEP CONVULUTION\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten # are the layer we are using\n",
    "from keras.layers import Conv2D #\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator #pre-processor imagedata generating\n",
    "\n",
    "\n",
    "\n",
    "train_dir = 'data/train' #trainin model - directory\n",
    "val_dir = 'data/test' #validate\n",
    "\n",
    "# generate the data set from the image file in directory in c=scal 1 to 255 since in picles, its an array from 1 and 255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255) \n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(48,48),\n",
    "    \n",
    "    #in ml, no. of training examples utilised in one iteration\n",
    "    batch_size=64,\n",
    "    \n",
    "    # gray scale, black and white scale\n",
    "    color_mode=\"grayscale\",\n",
    "    \n",
    "    #since it has classsification of happy, sad,etc.\n",
    "    class_mode='categorical')\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(48,48),\n",
    "    batch_size=64,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode='categorical')\n",
    "\n",
    "\n",
    "# creating model\n",
    "# we are initialiding model(Sequential model)\n",
    "# using keras we can use sequential and functional\n",
    "# seq - creat layer by layer, but limit in depth, cannot share layers or multiple input or output\n",
    "# fun - more flexible, find layers, find next layers, more complex\n",
    "emotion_model = Sequential()\n",
    "\n",
    "# adding layers\n",
    "\n",
    "# special type of nural network, used for image recognision and classification\n",
    "# convilution is simple applicatio of a flilter to an input that results in activation\n",
    "# some input some thrushhold, if input meets thurshhold then there is activation\n",
    "# repeated application of the same filter to an input results in map of optimation which is call feater map\n",
    "# if input goes in , that repeats in certain way it forms map\n",
    "# indicating location and strangs of a detected feter in the input, in our case images\n",
    "\n",
    "# first layer 2D convulutional layer\n",
    "# creats a convulutional kernel that is convolved with layer of input to form output\n",
    "# 32-input shape\n",
    "# hight, width-3,3\n",
    "# represents pictures, also grayscale images\n",
    "# relu- rectifier, which is an activation function used in nn(there is math)\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "\n",
    "#maxpooling2d - down sample our data,(to include less data, smaller subset of data)\n",
    "# training on a disproportinaly low subset of the classic sample\n",
    "# reverse - up waiting, adding a sample way to down sampleway class\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#droupout - randomly set input units to 0, with frequence rate at each step during the training time\n",
    "# helps to prevent over fitting\n",
    "# over fitting - a model that learned the noice instead of the signal that is considerd to be required signal forming\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "# have to flatten before turn all the neurons , 1024 in to 7(below values)\n",
    "# \n",
    "emotion_model.add(Flatten())\n",
    "\n",
    "# Dense - it is a hidden layer which has 1024 neurons/units\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "\n",
    "# 7 neurons used cauese it tells exact number of possiblities i.e. 7 emotions\n",
    "emotion_model.add(Dense(7, activation='softmax')) \n",
    "\n",
    "\n",
    "\n",
    "#compile model\n",
    "# loss is degree of error\n",
    "# model strive to minimize loss rather to increase accuracy\n",
    "# optimizer adem - complex\n",
    "# categorical cross - type the matrix how we specy the loss\n",
    "emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001, decay=1e-6), metrics=['accuracy'])\n",
    "\n",
    "# fit it\n",
    "emotion_model_info = emotion_model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=28709 // 64, # bentch size - 64\n",
    "    epochs=10, # no. of iterations to be don\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=7178 // 64)\n",
    "\n",
    "emotion_model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93808457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "import cv2\n",
    "from PIL import Image, ImageTk # we are inporting images using tkintre  \n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import threading\n",
    "\n",
    "emotion_model = Sequential()\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "#loading the weights\n",
    "emotion_model.load_weights('model.h5')\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "\n",
    "#creating dictionary for all emotion\n",
    "emotion_dict = {0: \"Angry\", 1:\"Disgusted\", 2: \"Fearful\", 3:\"Happy\", 4:\"Neutral\", 5:\"Sad\", 6:\"Surprised\"}\n",
    "\n",
    "#os library, path directiory name,absolut path of te emojies\n",
    "cur_path = os.path.adirname(os.path.abspath(_file_))\n",
    "\n",
    "emoji_dist={0:cur_path+\"/emojis/angry.png\", 1:cur_path+\"/emojis/disgusted.png\", 2:cur_path+\"/emojis/fearful.png\", 3:cur_path+\"/emojis/happy.png\", 4:cur_path+\"/emojis/neutral.png\", 5:cur_path+\"/emojis/sad.png\", 6:cur_path+\"/emojis/surprised.png\"}\n",
    "\n",
    "global last_frame1\n",
    "last_frame1 = np.zeroes((480, 640, 3), dtype=np.unit8)\n",
    "global cap1\n",
    "show_text=[0]\n",
    "global frame_number\n",
    "\n",
    "\n",
    "\n",
    "def show_subject():\n",
    "    \n",
    "    #open the video, path of video\n",
    "    # 0 - web cam access\n",
    "    cap1=cv2.VideoCapture(0)\n",
    "    \n",
    "    #if camera open or not\n",
    "    if not cap1.isOpenened():\n",
    "        print(\"Can't open the camera\")\n",
    "    \n",
    "    global frame_number\n",
    "    \n",
    "    #max no. of frames\n",
    "    length = int(cap1.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_number+=1\n",
    "    if frame_number >= length:\n",
    "        exit()\n",
    "        \n",
    "    # seting the next fram to increment\n",
    "    cap1.set(1, frame_number)\n",
    "    \n",
    "    # fram - it is an array\n",
    "    # flage - check if we read something or not\n",
    "    flag1, frame1 = cap1.read()\n",
    "    \n",
    "    #resizing\n",
    "    frame1= cv2.resize(frame1,(600,500))\n",
    "    \n",
    "    # a box near the face\n",
    "    # but open cv already has it\n",
    "    bounding_box= cv2.CascadeClassifier()\n",
    "    \n",
    "    #convert image to gray\n",
    "    #BGR to gray\n",
    "    # why we do?\n",
    "    # the memory usage, the bandwith, it will be fast processing time, less space\n",
    "    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    num_faces = bounding_box.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "    \n",
    "    for(x, y, w, h) in num_faces:\n",
    "        # creating rectangle\n",
    "        # recognising\n",
    "        cv2.rectangle(frame1, (x,y-50), (x+w, y+h+10),(250,0,0), 2)\n",
    "        roi_gray_frame = gray_frame[y:y+h, x:x+w]\n",
    "        # we have already trained the images dor size 48\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48,48)), -1),0)\n",
    "        prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        cv2.putText(frame1, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1,(255,255,255), 2, cv2.LINE_AA)\n",
    "        show_text[0]=maxindex\n",
    "    if flag1 is None:\n",
    "        print(\"Major error!\")\n",
    "    #getting our main window\n",
    "    elif flag1:\n",
    "        global last_frame1\n",
    "        last_frame1= frame1.copy()\n",
    "        pic= cv2.cvtColor(last_frame1, cv2.COLOR_BGR2GRB)\n",
    "        #we have array from 0 to 255\n",
    "        #generate image\n",
    "        img= Image.fromarray(pic)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        lmain.imgtk=imgtk\n",
    "        #since we are running in different thread we need to update our main thread\n",
    "        # update main thread where our tkinter where gui is runnung\n",
    "        root.update()\n",
    "        \n",
    "        #after - way to use tkinter to get around with blocking\n",
    "        #after the time 10 sec is assed it will run even i fthe main function is running\n",
    "        lmain.after(10, show_subject)\n",
    "    if cv2.waitkey(1) & 0xFF == ord('q'):\n",
    "        exit()\n",
    "\n",
    "def show_avatar():\n",
    "    frame2=cv2.imread(emoji_dist[show_text[0]])\n",
    "    pic2=cv2.cvtColo(frame2,cv2.Color_BGR2RGB)\n",
    "    img2=Image.fromarray(frame2)\n",
    "    imgtk2= ImageTk.PhotoImage(image=img2)\n",
    "    lmain2.imgtk2=imgtk2\n",
    "    lmain3.configure(text=emotion_dict[show_text[0]], font=('arial',45,'bold'))\n",
    "\n",
    "    lmain2.configure(image=imgtk2)\n",
    "    root.update()\n",
    "    lmain2.after(10, show_avatar)\n",
    "\n",
    "# main file\n",
    "if _name_ == '_main_' :\n",
    "    #specifying frame number, lagter we will read through our file, increment frame ny frame\n",
    "    frame_number= 0\n",
    "    root=tk.TK()\n",
    "    \n",
    "    # creating lables that are going to contain our images or video\n",
    "    lmain = tk.Label(master=root,padx=50,bd=10)\n",
    "    lmain2 = tk.Label(master=root,bd=10)\n",
    "    \n",
    "    #for quiting\n",
    "    lmain3 = tk.Label(master=root,bd=10,fg=\"#CDCDCD\",bg='black')\n",
    "    \n",
    "    # packing and placing\n",
    "    lmain.pack(side=LEFT)\n",
    "    lmain.place(x=50,y=250)\n",
    "    lmain3.pack()\n",
    "    lmain3.place(x=960,y=250)\n",
    "    lmain2.pack(side=RIGHT)\n",
    "    lmain2.place(x=900,y=350)\n",
    "\n",
    "    #specified title\n",
    "    root.title(\"Photo To Emoji\")\n",
    "    \n",
    "    #specified geomentry\n",
    "    root.geometry(\"1400x900+100+10\")\n",
    "    root['bg']='black'\n",
    "    \n",
    "    #exit button on bottom\n",
    "    exitButton = Button(root, text='Quit',fg='red',command=root.destroy,font=('arial',25,'bold')).pack(side = BOTTOM)\n",
    "    \n",
    "    #threading because main loop is blocking\n",
    "    # when get to main loop everything is goin to run\n",
    "    threading.Thread(target=show_subject).start()\n",
    "    threading.Thread(target=show_avatar).start()\n",
    "    root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
